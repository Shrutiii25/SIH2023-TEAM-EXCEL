import pandas as pd
import numpy as np 
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE
from xgboost import XGBClassifier
import seaborn as sns
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv("D:/Projects/EduDrop-Student-DropOut-Analysis/Algorithms & Analysis/assets/dataset.csv")

# Encode target variable
lbl = LabelEncoder()
df['Target'] = lbl.fit_transform(df['Target'])

# Calculate correlation matrix and select features
correlational_matrix = df.corr()
target_correlation = correlational_matrix['Target']
selectedcols = target_correlation[target_correlation.abs() > 0.1].index.tolist()  # Adjust threshold
df = df[selectedcols]

# Remove outliers using IQR
Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
mask = ~((df < Q1 - 1.5 * IQR) | (df > Q3 + 1.5 * IQR)).any(axis=1)
df = df[mask]

# Fill missing values with column means
df = df.fillna(df.mean())

# Split data into features and target
X = df.drop('Target', axis=1)
y = df['Target']

# Standardize features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Split the data
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=48)

# Apply SMOTE to the training data
sm = SMOTE(random_state=42)
Xtrain_res, ytrain_res = sm.fit_resample(Xtrain, ytrain)

# Define parameter grid for Random Forest
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'class_weight': ['balanced']
}

# Perform Grid Search
grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring='accuracy')
grid_search.fit(Xtrain_res, ytrain_res)

# Best model from grid search
best_model = grid_search.best_estimator_

# Evaluate with cross-validation
cv_scores = cross_val_score(best_model, X, y, cv=10, scoring='accuracy')
print(f"Cross-validation scores: {cv_scores}")
print(f"Mean CV accuracy: {cv_scores.mean()}")

# Train the best model on the resampled training data
best_model.fit(Xtrain_res, ytrain_res)

# Evaluate the model
train_score = best_model.score(Xtrain, ytrain)
test_score = best_model.score(Xtest, ytest)
print(f"Best Model - Training accuracy: {train_score:.2f}, Testing accuracy: {test_score:.2f}")

# Predictions and evaluation
ypred = best_model.predict(Xtest)
confusion = confusion_matrix(ytest, ypred)
report = classification_report(ytest, ypred)

# Print confusion matrix and classification report
print(confusion)
print(report)

# Plot confusion matrix
# plot_confusion_matrix(best_model, Xtest, ytest, display_labels=lbl.classes_, cmap=plt.cm.Blues)
# plt.show()

# Ensemble Model Example
ensemble = VotingClassifier(estimators=[
    ('lr', LogisticRegression(max_iter=1000)),
    ('rf', RandomForestClassifier(n_estimators=100, max_depth=10, class_weight='balanced')),
    ('xgb', XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='logloss')),
    ('ada', AdaBoostClassifier(n_estimators=100, algorithm='SAMME'))
], voting='soft')

# Train ensemble model
ensemble.fit(Xtrain_res, ytrain_res)

# Evaluate ensemble model
train_score_ensemble = ensemble.score(Xtrain, ytrain)
test_score_ensemble = ensemble.score(Xtest, ytest)
print(f"Ensemble - Training accuracy: {train_score_ensemble:.2f}, Testing accuracy: {test_score_ensemble:.2f}")
